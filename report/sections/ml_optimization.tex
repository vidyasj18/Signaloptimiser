\chapter{Signal Optimization using Machine Learning Models}
\label{ch:ml_optimization}

\section{Linear Regression}
\label{sec:linear_regression}

Linear regression is a statistical method that models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. The model finds the best-fitting straight line (or hyperplane in multiple dimensions) through your data points. For simple linear regression with one predictor, this looks like: $y = mx + b$, where $m$ is the slope and $b$ is the y-intercept. The model minimizes the distance between predicted and actual values.

Linear regression is a fundamental statistical modeling technique used in traffic optimization to establish predictive relationships between traffic variables and influencing factors. In traffic systems, it serves multiple critical functions including forecasting traffic volume based on temporal patterns such as time of day, day of week, weather conditions, and special events, which enables proactive signal timing adjustments and congestion management.

\section{Random Forests}
\label{sec:random_forests}

Random Forest is an ensemble machine learning algorithm that combines multiple decision trees to make more accurate and robust predictions through a ``wisdom of the crowds'' approach. The algorithm creates numerous decision trees during training, each built on a random subset of the data (bootstrap sampling) and considering only a random subset of features at each split. For predictions, it aggregates results from all trees - taking the majority vote for classification or the average for regression.

Random Forest is highly effective for traffic optimization due to its ability to capture complex, non-linear relationships between multiple traffic variables such as historical patterns, weather conditions, accidents, and real-time sensor data.

\section{Webster's Method}
\label{sec:webster_method}

The Webster method, also known as the Webster's method of apportionment or in traffic engineering as Webster's delay formula, is used to optimize traffic signal timing by minimizing average vehicle delay at intersections. In signal control, it calculates optimal cycle length based on traffic flows from different approaches, balancing the trade-off between minimizing stops and reducing overall waiting time to improve intersection efficiency and throughput.

In this project Webster method was used to train the Machine learning models so that we can optimize and predict the traffic values.

\section{Data Source and Input}
\label{sec:ml_data_source}

The machine learning models receive their input data from the automated YOLOv8-based vehicle detection system developed in Stage 1 of the workflow. Traffic videos recorded at each intersection approach are processed through the Streamlit application, which uses YOLOv8 to detect and classify vehicles (cars, buses, two-wheelers, trucks), count them accurately using virtual stoplines and ROI masks, and convert the counts into Passenger Car Units (PCUs) using IRC standard conversion factors. 

The output of this detection stage is saved as a JSON file (\texttt{intersection\_summary.json}) containing PCU values for each approach (N, S, E, W). This JSON file serves as the primary input to both the Webster-based calculations and the ML-based prediction models in Stage 2, ensuring consistent data input across all optimization methods and enabling fair comparison between traditional and machine learning approaches.

\section{Traffic Signal Optimization Formulas}
\label{sec:optimization_formulas}

\begin{table}[h]
\centering
\caption{Traffic Signal Optimization Formulas}
\label{tab:optimization_formulas}
\begin{tabular}{|l|l|p{6cm}|}
\hline
\textbf{Formula} & \textbf{Equation} & \textbf{Description} \\
\hline
Optimum Cycle Time & $C_o = \frac{1.5L + 5}{1 - y}$ & Calculates the optimum cycle length. $L$ = lost time, $y$ = summation of the critical flow ratio at all phases \\
\hline
Green Time Allocation & $G_a = \frac{y_a}{y} \times (C_o - L)$ & Allocates green time to each lane based on traffic demand \\
\hline
\end{tabular}
\end{table}

\section{Data Set Used}
\label{sec:dataset}

While Webster's method provides a mathematically sound approach to signal timing, it assumes perfect, noise-free traffic counts. To create a more realistic and robust training dataset, we generated synthetic data using Webster's equations but incorporated real-world traffic patterns and variations that actual intersections experience.

\subsection{Synthetic Dataset Generation with Real-World Patterns}

A comprehensive synthetic dataset of 3000 samples was created, each representing a unique traffic scenario. Rather than generating uniform traffic patterns, the dataset incorporates six key real-world factors:

\subsubsection{Time-of-Day Effects}

Traffic volumes vary significantly throughout the day. The dataset includes:
\begin{itemize}
    \item Morning peak (7-9 AM): 1.4$\times$ traffic multiplier
    \item Evening peak (5-7 PM): 1.5$\times$ traffic multiplier
    \item Late night (11 PM-5 AM): 0.3$\times$ traffic multiplier
    \item Normal hours: 1.0$\times$ baseline traffic
\end{itemize}

\subsubsection{Weather Impact on Capacity}

Adverse weather conditions reduce saturation flow rates and affect driver behavior:
\begin{itemize}
    \item Clear weather: 1.0$\times$ baseline capacity
    \item Rain: 0.85$\times$ capacity (reduced visibility, cautious driving)
    \item Heavy rain: 0.65$\times$ capacity (significant visibility reduction)
    \item Fog: 0.75$\times$ capacity (limited visibility)
    \item Snow: 0.60$\times$ capacity (slippery conditions, extreme caution)
\end{itemize}

\subsubsection{Special Events}

Random events (10\% probability) cause sudden traffic surges:
\begin{itemize}
    \item Event occurrence: 1.3$\times$ traffic on affected approaches
    \item Directional bias: Events create asymmetric demand patterns
    \item Real-world examples: Sports events, concerts, accidents, festivals
\end{itemize}

\subsubsection{Day-of-Week Patterns}

Traffic behavior differs between workdays and weekends:
\begin{itemize}
    \item Weekends: 0.8$\times$ traffic (reduced commuting)
    \item Weekdays: 1.0$\times$ traffic (regular commuting patterns)
\end{itemize}

\subsubsection{Directional Bias}

Real intersections rarely have perfectly balanced traffic:
\begin{itemize}
    \item Random variations: -20\% to +30\% per approach
    \item Morning patterns: More inbound to city center
    \item Evening patterns: More outbound from city center
\end{itemize}

\subsubsection{Capacity Variations}

Saturation flow rates vary based on intersection geometry and conditions:
\begin{itemize}
    \item Base saturation: 1800 PCU/hour per lane
    \item Weather-adjusted: Reduced based on conditions
    \item Time-adjusted: Peak hour capacity constraints
\end{itemize}

\subsection{Dataset Structure}

Each training sample includes:

\textbf{Input Features:}
\begin{itemize}
    \item N, S, E, W: PCU values for each approach (extracted from YOLOv8 automated vehicle detection in real-world application, or with real-world skew applied in synthetic training data)
    \item NS, EW: Combined directional flows
    \item hour: Time of day (0-23)
    \item weather indicators: Binary flags for different weather conditions
    \item has\_event: Boolean for special events
    \item is\_weekend: Boolean for weekend patterns
    \item day\_of\_week: 0-6 (Monday-Sunday)
\end{itemize}

\textbf{Output Labels:}
\begin{itemize}
    \item cycle: Optimal cycle length (seconds) calculated with real-world adjustments
    \item $g_N, g_S, g_E, g_W$: Green times for each approach (seconds)
    \item total\_delay: Actual delay incorporating real-world factors
\end{itemize}

\subsection{Training Process}

The synthetic dataset was generated using a two-step process:
\begin{enumerate}
    \item \textbf{Base Calculation:} Apply Webster's formulas to compute theoretical optimal timings
    \item \textbf{Real-World Adjustment:} Apply time-of-day, weather, event, and directional bias factors to create realistic variations
    \item \textbf{Delay Calculation:} Compute actual delays using adjusted capacity and flow values
\end{enumerate}

This approach ensures the ML models learn not just Webster's mathematical relationships, but also how to adapt to real-world conditions that Webster's formula doesn't account for. The dataset's compatibility with real-world sensor data makes it suitable for future retraining when actual intersection data becomes available.

\section{Rationale for Integrating Machine Learning with Webster's Method}
\label{sec:ml_webster_rationale}

\subsection{Teacher-Student Approach}

\textbf{Teacher (Webster):} We generate many synthetic traffic scenarios (N, S, E, W) and compute ``ideal'' labels using Webster's proportional splits and cycle formula. This encodes accepted traffic engineering behavior.

\textbf{Student (ML):}
\begin{itemize}
    \item Linear Regression learns a simple mapping from (NS, EW) $\rightarrow$ cycle, close to Webster's relationship in the operating range.
    \item RandomForest learns a robust mapping from (N, S, E, W) $\rightarrow$ ($g_N$, $g_S$, $g_E$, $g_W$), capturing mild nonlinearities and being resilient to noise.
\end{itemize}

\subsection{Model Selection}

\subsubsection{Cycle Length Model -- Linear Regression}

\textbf{Inputs:} (NS, EW) totals; \textbf{Output:} cycle length.

\textbf{Rationale:} Webster's cycle is monotonic in the overall loading. A linear regressor provides an interpretable, low-variance fit that closely matches the synthetic Webster mapping while tolerating small measurement noise.

\textbf{Benefits:} Interpretable, fast, and sufficient because the mapping from aggregate load to cycle is close to linear in the operational range after clamping and capping $Y$.

\subsubsection{Green Split Model -- RandomForestRegressor}

\textbf{Inputs:} (N, S, E, W); \textbf{Output:} per-approach green seconds ($g_N$, $g_S$, $g_E$, $g_W$).

\textbf{Rationale:} Splitting greens between approaches can have mild nonlinearities (e.g., diminishing returns, asymmetric sharing in noisy detections). A Random Forest captures interactions and outliers better than a pure linear rule, without heavy hyper-parameter tuning.

\textbf{Training:} Synthetic ``teacher'' labels are generated by Webster proportional splits (data augmentation across a wide demand range). The forest is then normalized at inference so $\Sigma$ greens $\approx$ effective green ($C - L$), preserving feasibility.

\textbf{Benefits:} Robust to jitter/outliers in detections, handles asymmetric loads gracefully, and remains stable under noise; easy to retrain if you collect local data.

\newpage

