\chapter{Signal Optimization using Machine Learning Models}
\label{ch:ml_optimization}

\section{Linear Regression}
\label{sec:linear_regression}

Linear regression models how one quantity changes with another by fitting a straight line (or a hyperplane). In its simplest form: $y = mx + b$, with $m$ as slope and $b$ as intercept. The goal is to keep predictions close to actual values on average.

It’s a solid baseline for traffic problems—useful for predicting volumes from time-of-day, day-of-week, or weather patterns, and for making quick, interpretable adjustments to timing plans.

\section{Random Forests}
\label{sec:random_forests}

Random Forest is an ensemble of decision trees—many simple models that vote together. Each tree sees a random slice of the data and features; the final prediction averages across them for stability.

It works well when relationships are messy or non-linear—think mixed vehicle classes, weather, incidents, and sensor noise.

\section{Webster's Method}
\label{sec:webster_method}

Webster’s method is a staple in traffic engineering for minimizing average delay at intersections. It calculates an optimal cycle length from flows on different approaches and balances the trade-off between stops and waiting time.

Here, we used Webster both as the optimization baseline and as a “teacher” to help train ML models to predict feasible timings.

\section{Data Source and Input}
\label{sec:ml_data_source}

Inputs come from the automated YOLOv8-based detection pipeline. Videos from each approach are processed in the Streamlit app to detect and classify vehicles, count them with virtual stoplines and ROI masks, and convert counts to PCUs using IRC factors. 

The result is a JSON file (\texttt{intersection\_summary.json}) with PCUs for N, S, E, and W. That same file feeds both Webster calculations and ML models, keeping inputs consistent for fair comparison.

\section{Traffic Signal Optimization Formulas}
\label{sec:optimization_formulas}

\begin{table}[h]
\centering
\caption{Traffic Signal Optimization Formulas}
\label{tab:optimization_formulas}
\begin{tabular}{|l|l|p{6cm}|}
\hline
\textbf{Formula} & \textbf{Equation} & \textbf{Description} \\
\hline
Optimum Cycle Time & $C_o = \frac{1.5L + 5}{1 - y}$ & Calculates the optimum cycle length. $L$ = lost time, $y$ = summation of the critical flow ratio at all phases \\
\hline
Green Time Allocation & $G_a = \frac{y_a}{y} \times (C_o - L)$ & Allocates green time to each lane based on traffic demand \\
\hline
\end{tabular}
\end{table}

\section{Data Set Used}
\label{sec:dataset}

Webster’s method is mathematically sound, but it assumes clean counts. To make training more realistic, we generated synthetic data using Webster’s equations and layered in real-world patterns and variability seen at actual intersections.

\subsection{Synthetic Dataset Generation with Real-World Patterns}

We built a 3,000-sample synthetic dataset, each a unique scenario. Instead of uniform patterns, we varied six key factors:

\subsubsection{Time-of-Day Effects}

Traffic varies by time of day. We included:
\begin{itemize}
    \item Morning peak (7-9 AM): 1.4$\times$ traffic multiplier
    \item Evening peak (5-7 PM): 1.5$\times$ traffic multiplier
    \item Late night (11 PM-5 AM): 0.3$\times$ traffic multiplier
    \item Normal hours: 1.0$\times$ baseline traffic
\end{itemize}

\subsubsection{Weather Impact on Capacity}

Weather affects capacity and behavior:
\begin{itemize}
    \item Clear weather: 1.0$\times$ baseline capacity
    \item Rain: 0.85$\times$ capacity (reduced visibility, cautious driving)
    \item Heavy rain: 0.65$\times$ capacity (significant visibility reduction)
    \item Fog: 0.75$\times$ capacity (limited visibility)
    \item Snow: 0.60$\times$ capacity (slippery conditions, extreme caution)
\end{itemize}

\subsubsection{Special Events}

Occasional events (10\% chance) create surges:
\begin{itemize}
    \item Event occurrence: 1.3$\times$ traffic on affected approaches
    \item Directional bias: Events create asymmetric demand patterns
    \item Real-world examples: Sports events, concerts, accidents, festivals
\end{itemize}

\subsubsection{Day-of-Week Patterns}

Weekdays vs weekends differ:
\begin{itemize}
    \item Weekends: 0.8$\times$ traffic (reduced commuting)
    \item Weekdays: 1.0$\times$ traffic (regular commuting patterns)
\end{itemize}

\subsubsection{Directional Bias}

Flows are rarely perfectly balanced:
\begin{itemize}
    \item Random variations: -20\% to +30\% per approach
    \item Morning patterns: More inbound to city center
    \item Evening patterns: More outbound from city center
\end{itemize}

\subsubsection{Capacity Variations}

Capacity varies with geometry and conditions:
\begin{itemize}
    \item Base saturation: 1800 PCU/hour per lane
    \item Weather-adjusted: Reduced based on conditions
    \item Time-adjusted: Peak hour capacity constraints
\end{itemize}

\subsection{Dataset Structure}

Each training sample includes:

\textbf{Input Features:}
\begin{itemize}
    \item N, S, E, W: PCU values for each approach (from YOLOv8 in real data, or with realistic skew in synthetic data)
    \item NS, EW: Combined directional flows
    \item hour: Time of day (0-23)
    \item weather indicators: Binary flags for different weather conditions
    \item has\_event: Boolean for special events
    \item is\_weekend: Boolean for weekend patterns
    \item day\_of\_week: 0-6 (Monday-Sunday)
\end{itemize}

\textbf{Output Labels:}
\begin{itemize}
    \item cycle: Optimal cycle length (seconds), adjusted with real-world factors
    \item $g_N, g_S, g_E, g_W$: Green times for each approach (seconds)
    \item total\_delay: Actual delay incorporating real-world factors
\end{itemize}

\subsection{Training Process}

We generated the dataset using:
\begin{enumerate}
    \item \textbf{Base Calculation:} Apply Webster's formulas to compute theoretical optimal timings
    \item \textbf{Real-World Adjustment:} Apply time-of-day, weather, event, and directional bias factors to create realistic variations
    \item \textbf{Delay Calculation:} Compute actual delays using adjusted capacity and flow values
\end{enumerate}

This way, models learn both Webster’s relationships and how to adapt when the real world deviates. Because the features align with field data, the models can be retrained later with actual intersection datasets.

\section{Rationale for Integrating Machine Learning with Webster's Method}
\label{sec:ml_webster_rationale}

\subsection{Teacher-Student Approach}

\textbf{Teacher (Webster):} We generate many synthetic scenarios (N, S, E, W) and compute “ideal” labels via Webster’s cycle and proportional splits—encoding accepted engineering practice.

\textbf{Student (ML):}
\begin{itemize}
    \item Linear Regression learns a simple mapping (NS, EW) $\rightarrow$ cycle, close to Webster in the operating range.
    \item RandomForest learns a robust mapping (N, S, E, W) $\rightarrow$ ($g_N$, $g_S$, $g_E$, $g_W$), capturing mild nonlinearities and resisting noise.
\end{itemize}

\subsection{Model Selection}

\subsubsection{Cycle Length Model -- Linear Regression}

\textbf{Inputs:} (NS, EW) totals; \textbf{Output:} cycle length.

\textbf{Rationale:} Webster’s cycle increases with overall loading. A linear regressor is interpretable, low variance, and matches the synthetic mapping while tolerating small noise.

\textbf{Benefits:} Interpretable and fast; the aggregate-load-to-cycle mapping is close to linear in the operating range after clamping and capping $Y$.

\subsubsection{Green Split Model -- RandomForestRegressor}

\textbf{Inputs:} (N, S, E, W); \textbf{Output:} per-approach green seconds ($g_N$, $g_S$, $g_E$, $g_W$).

\textbf{Rationale:} Green splits can be mildly non-linear (diminishing returns, asymmetric sharing with noisy detections). A Random Forest captures interactions and outliers better than a strict linear rule, without heavy tuning.

\textbf{Training:} Synthetic “teacher” labels come from Webster’s splits, augmented over wide demand ranges. At inference the forest is normalized so $\Sigma$ greens $\approx$ effective green ($C - L$), keeping plans feasible.

\textbf{Benefits:} Robust to jitter and outliers, handles asymmetric loads gracefully, stays stable under noise, and is easy to retrain with local data.

\newpage

